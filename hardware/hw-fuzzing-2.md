# 处理器验证与处理器模糊测试（二）

本章节我们介绍测试样例生成的一些基本方法，以及分析其行之有效的原理。

## 基本测试方法

我们以下面这个功能模块为例子设计测试。这个功能模块（不需要在意是软件还是硬件）接受两个输入，一个输入的范围是 0-7,一个输入范围是 0-15，输出是一个值，我们用不同的颜色表示不同值的输出。现在我们需要设计充分的测试样例，来测试这个功能模块。当然，这里的“充分”与否是一个充满主观的词汇，其充分性视现状和我论述的需求而定。
![简单功能点案例](img/coverage.png)

### 遍历测试
最简单也是最有效的方法，我们遍历所有的输入组合。这样我们就覆盖了所有的功能测试点，用朴素的方法完美解决了这个问题。唯一的问题在于这个方法生成的测试样例集合巨大，可扩展性级差。现在输入的范围是(0-7, 0-15)，共计 128 个测试样例，但是如果是(0-79,0-159)，那就是 12800 个测试样例。其实上万个测试样例也还好，但即使只是简单的 64 位输入，他的全集也有 2^64 次方，那我们也不可能在有生之年完成验证，便显得没有意义了。因此，这种遍历测试仅适用于最基本模块、小型模块的测试。
![庞大功能点案例](img/coverage2.png)

对于(0-79,0-159)，遍历所有的测试已经比较困难了（这里我们假设受到硬件设备的约束无法遍历，但是在实际情况中一万个测试样例还是比较适中的测试规模），所以我们被迫选择测试的部分输入组合来试图获得较好的验证结果。即使从节约成本的角度考虑，用少很多的测试样例就可以得到比较高的验证效果，也是有意义的。但是怎么选择这部分输入呢？怎样用尽可能少的测试样例取得尽可能好的验证效果呢？（近似算法问题）

### 随机测试
最简单的方法就是随机生成测试输入，这种生成方法是最简单的。很多时候，大家会对这种随机算法保持高度的怀疑，因为这样就是把验证结果的好坏交给了天老爷。但是实际上，随机算法的输出一般是均匀分布的，理论上只要测试时间足够就就可以覆盖所有的输入组合，或者说只要执行时间够长，一个测试点被覆盖的概率就会足够高。因此从理论上讲，在足够长的时间内，随机测试的效果和遍历测试是等价的。但是在实际上，这个时间是很有限的，因此必然只有一部分测试点可以被覆盖。下图为随机测试 1280 个样例的覆盖结果，虽然只测试了 10% 的功能点，但是基本覆盖了所有的重要区域。总的来说，随机测试的算法和工程部署足够简单，测试样例生成效率极高，并且在实践中能取得不错的效果；缺点则是可控性较差，而且测试结果的可信度存疑。
![随机测试覆盖情况](img/coverage-random.png)

### 边界测试
我们观察一下输出的细节。在(19,20)和(20,20)的位置，我们可以看到输出的颜色从紫色变为了绿色，发生了输出的突变。这里对于边界判断就是边界条件，19 或者说 20 将输出左侧和右侧划分为了两大类，因此我们需要检测这个边界附近的输出结果，判断区域边界的划分是否正确。这种边界测试在整体测试点中的占比显然是很小的，所以我们即使遍历验证开销也会小很多。考虑到边界更具有临界突变的特殊性，并且占比很小，如果我们简单用随机的方法很难覆盖到这些特殊点，因此更适用于手工定制。当然如何分析模块特点，确定和构造这些边界，也是一个复杂、不通用的问题。边界测试的结果如下：
![边界测试覆盖情况](img/coverage-edge.png)

如果边界测试的集合仍然太大了，我们仍然可以用随机的方式对边界测试点进行测试，来逼近期待的效果。
![随机边界测试覆盖情况](img/coverage-edge-random.png)

## 边界信息模型
这是我为了分析随机测试和边界测试内在的机理而建立的模型，其不合理之处请批评指正，但也请尊重我的学术版权。

编程的过程就是用一种语言描述功能信息的过程。我们定义功能 F 蕴含的信息是 I，然后用语言 L 的元素 l 对功能 F 进行描述。l 的每个字符 c 承载了 I 的部分信息 i。我们验证的目标就是验证 l 的各个子部分的蕴含的信息 i 是否正确。

我们可以用简单但庞大的真值表对功能进行描述，每个真值表表项表述一个输入到输出的关系。例如，对于上述的功能，我们可以用如下的 C 代码进行编程描述，可以看到二维数组的每个表项描述一对输入输出关系，我们假设描述问题所需的信息含量是单位 I，这里用 12800 个表项来完成问题描述，所以每个表项的信息含量仅为 I/12800。可见这种描述方法所用的 l 字符长度极大，每个子部分蕴含的信息量极小。
```C
代码一：
assume(0 <= i <= 159);
assume(0 <= j <= 79);
int array[80][160] = {
    ...
};
int o = array[j][i];
```
为了检验所有输入输出的关系的正确性，需要检验所有 12800 的输入组合，不然无论如何是不能完全测试正确的，正是因为每个子部分的信息量含量极低，所以我们每个测试可以验证的信息量极低，进而导致测试效率很低。对于这种基于点对的功能描述方法，只有遍历测试才可以测试完全（所以理论上随机测试、边界测试不是万能的，知识不得以而为之的折衷选择）。

这里我们引入“边界信息模型”作为功能信息描述的建模方式，并以此论证一些测试方式的有效性。计算机编程的问题，可以被看作一个分类问题，我们把输入划分为若干个子类别，每个类别有一致的输出方式，这个问题的信息就可以由各个类别的划分边界和每个类别的表示方式表示。我们上述案例明显就是一个分类问题，我们用四条边界把模型划分为两个区域，一个区域输出绿色，一个区域输出紫色。我们可以用 if-else 语句构造分类的边界，并给每个区域赋值对应的颜色：
```C
代码二：
if(20 <= i && i < 80 && 20 <= j && j < 60){
    o = 绿色;
} else {
    o = 紫色;
}
```
假设我们的建模和功能是一致的，我们现在只需要检验边界是否构造正确，以及每个区域的表达方式是否分配正确即可。对于边界，我们就可以手动构造边界测试，检验边界位置是否和理论模型保持一致；对于每个类别，我们需要检测类别是否一致，而不需要检测所有的输入输出点对。这里我们用四个边界+两个区域表示完成了对功能的描述，因此每个子部分表述的信息量极高；我们验证每个子部分可以覆盖的信息量极大，所以验证效率极高。

实际上，代码一也可以看作一种边界模型。数组的每个表项都可以看作是一个单独的子部分，所以这个模型相当于把问题分割为 12800 类，然后给每一类分配绿色还是紫色。因为分类众多，这就导致最后验证的边界数量和类表示方式众多，每个子部分信息含量低，验证效率低。

我们编程的时候会自觉的将问题进行高度抽象和归类，例如上述例子，根据区域的特点，可以将 12800 个子类合并为 1000 类、10 类乃至 2 类，这种信息压缩的方式，大大提高了信息表达的效率，也包括代码开发的效率，进而提高了验证的效率。可见，代码开发效率、验证效率是和问题的抽象程度、模块化程度正相关的。

### 边界测试有效性的论证

我们提到只要检测四条边和两个区域的正确性，就可以证明上述功能的有效性，这里涉及到为什么边和区域表达的正确性可以快速验证。
```C
代码二：
if(20 <= i && i < 80 && 20 <= j && j < 60){
    o = 绿色;
} else {
    o = 紫色;
}
```

这个快速验证不是无条件正确的，是基于代码高度的模块化和子模块先验的正确性这两个前提上的。这里我们需要检验绿色的矩阵区域都是被设置为绿色的，这里的表达式是`o = 绿色`，因为`=`可以将任何一个输入位置的输出都变为对应的值，所以只要有一个位置被验证颜色正确，那么所有位置都可以被验证颜色正确。再比如我们需要验证 20 是不是被正确划分，只需要检验附近的一组点就可以了，因为`20 <= i`的边界 19、20、21 都被验证边界正确了，无论了 j 怎么变化都可以边界正确。但这些前提不是天然成立的，而是基于`=, <=, &&, <`等算子的正确性和模块化的前提下。如`20 <=`本质上是一个输入范围为 0~159 的算子，如果是我们自己用 cmos 管构，是需要充分测试所有 160 情况的，但是因为默认提供的`<=`模块是对的（或者说历史上验证了他的正确性），所以这么多的状态只需要检验 20 的边界即可，需要验证的状态就从 160 种压缩为了 2 种。

正是这种模块化、层次化的设计提高了信息的表达能力，提高了验证的效率。模块化将长长的函数语句转化为简单的函数调用，层次化将小模块组装为大模块，从而不断提高子部分的信息表达能力，不断降低需要验证的信息空间。比如我们首先验证加减乘除等基本算子的正确性，这样就可以验证所有被使用算子的正确性；之后验证基本算子构造的基本函数的正确性，此时只需要验证算子的组合，而不是需要再验证算子本身，相当于基本算子压缩了输入分类，所以可以大大降低待验证的空间；然后用基本函数构造复杂函数，依次了类推。

### 随机测试对边界测试的补充

这是出于四方面的考虑。

* 区域内部表达验证：边界测试需要检测边界内部地实现是否正确，虽然因为高度模块化的存在，一个点的正确可以推理得到所有点的正确，但是还是随机采样测试其中一个点的。

* 对分类构造的补充：想要构造正确的边界测试，需要对问题的边界模型有深刻的认识。这不仅仅需要对问题有充分的认识，还要对代码实现有充分的认识。下图的代码中，输入数据依次发生了三次变换，然后类别依次可以分为 2 类、4 类、6 类和 2 类。所以为了充分进行边界验证，我们其实应该针对 6 类的边界进行测试。因此，除非我们对代码逻辑有深刻的认识，不然是无法构造完备的边界测试的。对于复杂的代码逻辑，验证者难以构造正确的边界，更不要说有时候没有源代码。这个时候就可以考虑用随机测试尝试覆盖验证者无法意识不到的边界。

![边界在程序内部的变化](img/func-kind.png)

* 对复杂边界的妥协：对于复杂的测试逻辑而言，其内部的分类情况是非常复杂的。也许一个 12800 输入组合的类别内部真的有 10000 种组合。这个时候边界描述的信息含量会快速上升，以至于和点阵描述相比信息含量没有明显优势，甚至更差，那只能用随机测试的方式在测试效果和测试效率之间寻求妥协了。

* 检验模块正确性假设：我们之前说边界测试的正确性是基于模块化假设的，例如我们默认基本算子、基本函数是正确的，但是这也是有概率不成立的，比如流片失败、芯片老坏、磁盘损坏、高能粒子流等影响，所以在高正确性要求的情况下，随机测试有保底的作用。

![结合两种基本方法](img/coverage-together.png)

我们最后小结一下二者的优缺点:
* 随机测试：生成高效快速、不需要额外知识和人力、针对性弱、泛化性好、适用于低模块化场景和黑盒场景
* 边界测试：需要额外知识和人力成本、针对性强、泛化性差、适用于高度模块化场景

## 模糊测试

### 信息量搜索
我们继续沿用边界信息模型作为信息量的分布模型，认为不同输出颜色的输入是属于不同类别的，类别之间的分界线是信息量含量很高的结构。测试分界线周围的功能点信息验证效率高，测试分界线内部的功能点信息验证效率低。我们理想的测试生成是对信息量高的区域进行较多的测试，对于信息量低的区域进行较少的测试，但很多时候我们缺少对模块内部构造的认识（比如没有源代码，比如代码结构过于复杂），因此我们可以考虑用一个搜索算法慢慢寻找模块内部信息量高的区域，勾勒出信息量的分布（在边界信息模型中就是边界地分布情况），然后根据信息量的多少进行不同程度的测试。

现在我们设计了如下的测试算法：

1. 随机生成一个测试输入作为搜索的原点，检查测试输入的颜色信息
2. 以上个测试输入做为起点随机游走一段距离（如 0.1），然后检测颜色信息变化
    * 如果颜色信息没有变化，就说明现在是在一个类别内部进行测试，信息量较低，甚至没有提供新的信息量
    * 如果颜色信息发生变化，就说明现在跨越了类别的边界，发现了类别边界这个信息量较高的结构，提供了新的信息量
3. 如果发现信息量较高的结构，就继续重复 2 的游走过程，尽可能地测试边界附近的测试点
    * 如果 3 可以一直重复，就说明一直在围绕边界进行测试，可能在原地打转，可能沿着边不断测试
4. 如果连续一段时间（比如 20 次突变）都没有找到新的边界（没有提供足够的信息量），就会进入 1
    * 说明附近大概率没有什么新的有待发掘的信息量，所以去一个较远的处女地进行信息量探索

我们现在用这个算法进行测试，下图分别是第 100、1000、10000 次测试的执行结果：
![100次测试结果](img/coverage-basic_fuzz.png)
![1000次测试结果](img/coverage-basic_fuzz-1.png)
![10000次测试结果](img/coverage-basic_fuzz-2.png)

我们可以清晰地看到在色块的边界测试的规模比较密集，而在色块的内部测试比较均匀分散。前者逼近了我们需要的边界测试，后者逼近了我们需要测试的随机测试。这样我们就再没有先验知识保证的情况下哎，得到了和之前测试类似的效果。

### 通用算法

我们现在正式介绍通用的模糊测试算法：
1. 获得一组测试集合作为语料库 corpus
2. 挑选语料库的中的一个测试节点或者随机生成一个节点，进行突变 mutate
3. 用突变结果进行测试，反馈覆盖率 coverage
4. 如果突变提供了新的覆盖率就继续突变，并且将这些有意义的突变加入语料库
5. 如果突变长期没有提供新的覆盖率，就选择其他的语料或者随机生成新的输入，执行步骤 2。

**突变 mutate**：突变就是对输入进行修改，例如将 01010 翻转若干个 bit 得到 11110, 或者将 01010 加上一个值得到 01011，或者产生一个新的随机数等等。突变的过程，其实就是之前讲到的测试输入随机游走的过程，只不过随机游走是在 N 空间下走一个步长，而 bit 翻转是在 GP(2) 空间下走一个步长。设计难点在于如何设计突变规则，如怎么修改输入，何时修改输入等等。

**覆盖率 coverage**：在边界信息模型的语义下验证的信息量占总的信息量的比值。我们的信息量是 I，如果我们可以验证 0.75 I，那么覆盖率就是 0.75。我们的目的就是提高信息量验证的覆盖率，让覆盖率向 100% 靠拢。上述的搜索信息量的过程，其实就是寻找新的覆盖率、提高覆盖率的过程。设计难点在于如何设计评估信息量的覆盖率，不同的模型适用于不同的覆盖率测量方式。

**种子 seed**：在上述算法中，我们搜索信息量的过程就是从一个测试点出发，然后开始随机游走。这个起始点的位置对于寻找信息量是有很大帮助的，如果这个点在边界附近，那么就可以很快找到很多信息量；如果点在色块内部远离边界，那么就很难找到信息量。所以如果可以有一组优秀的测试点作为起始测试，那么就可以大大提高信息量搜索的效率。起始点被称之为种子 seed，优秀的种子带来优秀的测试效果。设计难点在于寻找优秀的种子。

**语料库 corpus**：如果我们发现一个测试样例可以提供很高的信息量，那么这个测试样例本身就是一个优秀的测试点，我们可以收集这些测试点作为 benchmark 来进行回归测试。另一方面，这些测试点周围往往存在其他优秀的测试点，比如一个边界沿线都是优秀的测试点，所以我们可以收集这些测试点，然后在这些测试点的基础上进行游走，来提高信息量收集的效率。那个用于突变的测试集合就是语料库。设计难点在于寻找优秀的测试集合作为语料库。

### 二进制突变工具 AFL
我们现在介绍 SOTA 的软件模糊测试工具 AFL，我们通过介绍该工具拓宽大家对模糊测试各组件、测试流程的视野。

**突变策略**：AFL 针对二进制输入进行突变。例如我有一个模块的输入是 32 位的，那么 AFL 就对这个 32 位的二进制进行突变。突变策略诸如翻转 1、2、4、8、16、32 个 bit，加减一个随机数，产生一个随机数等。这种突变策略比较接近随机突变和近邻游走的组合，适用于通用场景。

**覆盖率评估**：软件是一个代码块组成的控制流图，代码块之间的边表示代码的控制流跳转关系。如果代码在执行的时候出现了新的代码块跳转路径，那么就是触发了新的控制流跳转条件，相当于越过了一个分类边界，找到了新的信息量。相对于用。因此 AFL 用这个代码块的跳转关系（block_a -> block_b），作为衡量覆盖率的指标。相比于直接用被执行的代码块 block_a，跳转的块之间的集合 {block_a, block_b}，用（block_a -> block_b）对信息的勾勒更为准确。

**覆盖率插桩**：直接执行程序并不能知道那些代码快被执行了，那我们如何得到控制流的覆盖率反馈呢？这就需要我们在代码中加入额外的插桩代码，当执行代码块间的跳转的时候也会执行这部分代码，然后记录对应的控制流跳转信息。我们需要用 AFL 对原代码进行编译，然后加入插桩的代码，block_a 跳转 block_b 替换为跳转到 block_a_b，block_a_b 将 a、b 的序号做一个 hash，表示是从 a 块跳转到 b 块，然后将覆盖率矩阵对应的 bitmap 设置为 1，这就记录了对应的调转情况，然后跳转到 block_b。这样就即不改变代码功能，也记录覆盖率情况。之后 AFL 根据 bitmap 执行突变策略。之所以使用 hash 是为了压缩覆盖率表示的空间，节约 bitmap 的空间。

```txt

    +---------------+           +---------------+
    |               |           |               |
    |    block_a    |           |    block_a    |-------------++
    |   j block_b   |           |   j block_a_b |             ||
    |               |           |               |            \||/
    +---------------+           +---------------+             \/
            ||                                       +------------------+
           \||/                                      |  block_a_b       |
            \/                                       |  index=hash(a,b) |
    +---------------+           +---------------+    |  cov[index] = 1  |
    |               |           |               |    |  j block_b       |
    |    block_b    |           |    block_b    |    +---------+--------+
    |               |           |               |<-------------+
    +---------------+           +---------------+

```

### 语义辅助突变

可以看到之前的输入突变策略基本是根据覆盖率反馈随机生成的，覆盖率策略只做到剪枝（放弃探索价值不大的区域）和选择突变激烈程度而已，除此之外并没有针对代码的结构融入更多的突变智慧。如果我们针对代码的特点设计对应的突变策略，往往可以提高测试的效果。所谓的代码特点就是描述功能的语言 l 的特点，也就是所谓的语义信息。

我们介绍一些语义辅助的设计思路，并且继续用边界信息模型对其有效性进行分析。下图是描述问题的代码 l 对应的边界信息模型，分为了足足13 个大类。如果我们对 l 的语义有足够深入的理解，那么我们就清晰的指导问题分为 13 类，然后大概率随机各个边界，小概率随机边界内部即可。这就是对于语义或者说代码结构、问题结构的最典型的利用。
![问题的语义模型](img/coverage-basic_fuzz.png)

但实际上我们对于问题的认识是比较浅薄，我们往往只是大致知道问题分为几个大类，但是即不知道大类下面细分的小类，也不知道每个类的边界在哪里。比如对于上述问题，我们知道有紫色、绿色、橙色、灰色四个大类，并且知道每个大类的大致分布位置；但是没有意识到其实绿色还细分为深浅不一的四个小类，也不知道四个大类的边界在哪里。那么我们怎么利用这些知识，在通用模糊测试的基础上做出一些改进呢？

首先是我们可以根据四个大类的大致位置设计语料库。因为紫色的区域比较大，所以随机搜索很容易将大多数的时间花费在紫色的区域，迟迟不能探索到另外三个区域。但如果我们为四个区域分别设置一个位置作为起始位置，那么就可以很快搜索到四个区域的典型部分，大大提高了前期探索的速度。

其次是我们之前的种子突变是直接随机的，所以种子的探索区域和各个类别的面积成正比。但是信息量并不一定是均匀分布的，边缘的信息量比较大，区域内部的比较小，我们可以用如下公式近似：
$$I = W_1 E + W_2 S, W_1 > W_2 $$
E 是边的长度，S 是区域的面积，W1 和 W2 分别是单位 E 和 S 的信息密度。我们先验的认为 W1 远大于 W2。 一个 9✖9 的区域面积虽然是 3x3 的 9 倍，但是边长只有 3 倍，因此信息量应该是在 3～9 之间，甚至远小于 9。所以根据面积生成，不如根据类别生成，这样随机种子的分布更接近信息量的部分，可以得到更好的测试效果。上图为应用语义前的结果，下图是应用语义后的结果，可以看到每个大类内部的各个小类的边界都得到了及其充分的测试。

![基于面积的随机](img/coverage-basic_fuzz-2.png)
![基于类别的随机](img/coverage-kind_fuzz-2.png)

我们希望突变的分布和信息量的分布可以高度一致，所以每个类的生成概率最好和 I 成正比。但是，我们观察公式，发现这里的 E、W1、S、W2 都是大小未知的，进而无法估计对应的 I。
$$I = W_1 E + W_2 S, W_1 > W_2 $$
但是我们可以用动态统计的方法来计算 I 的期望，从而用估算的 I 逼近需要的结果。例如用该大类获得的覆盖率除以该大类测试的次数作为覆盖率期望，然后根据贪心算法，每次挑选几个大类中期望最大的一类进行突变生成。
$$I_i = \frac{\bigcup_{j=0}^{n}{cov_{ij}}}{n}, i = argmax_{i}(I_i)$$
但其实随着模糊测试地进行，一个大类内部可以挖掘的覆盖率是在不断减少的，所以这个挖掘期望也是在不断下降的。如果覆盖率随测试次数的增长期望变化如下图所示。斜率较大的蓝线是平均的期望，但是我们期待的覆盖率增长速度其实是最高点的斜率，是远远小于平均值的。

![覆盖率增长变化曲线](img/coverage_curve.png)

因此我们实际上应该用一个局部的平均值来逼近斜率，然后替代全局平均值的使用，因此公式如下，w 为统计窗口的宽度。

$$I_i = \frac{\bigcup_{j=n-w}^{n}{cov_{ij}}}{n}, i = argmax_{i}(I_i)$$

## 图主动学习的启发

最近因为课程需要学习了一些图主动学习相关的论文，发现图主动学习技术和测试样例选择之间有可借鉴之处，机器学习对正确率的度量和处理器验证对硬件可信度的度量之间存在联系。我们现在来介绍一下对样例生成有启发的图主动学习背景和思路。

### 主动学习

常规的机器学习主要是以被动学习为主的。对于整个数据集的每个数据点，我们都手工打上标签，然后用于模型训练。但是如果数据集非常大的话，大标签和模型训练的成本会非常高，为了节约成本，我们希望可以选择尽可能少的标签数据，来达到尽可能高的训练效果。在我们日常生活的实践中，是有类似的情况作为理论的支撑的，就以刷题为例：一道题目会做之后，做其他同类型的题目除了提高熟练度，就学不到新的知识点了；难的题目会做了之后，做简单的题目也不会学到新的知识。所以没有必要把一个题库的所有题目都学完，将典型例题做完就好了。

基于信息量的主动学习技术认为，神经网络学习数据-标签就是学习数据背后的信息量，而不同的数据具有不同的信息量，相似的数据具有相似的信息量。下图是我们对一类数据-标签数据集的二图展示，每个点的颜色是对应的标签，每个点的位置是对应的向量空间分布。现在我们尝试挑选信息量丰富，有更高学习价值的数据。
![数据集二维图](img/dataset-graph.png)

比如说绿色数据集中的地方，我们可以挑选几个点作为绿色的典型情况，类似于经典例题，富有信息量，而其他的绿色的点因为有高度类似的信息量，重复学习新增的知识有限，所以就没有继续学习的必要了。这也说明信息密度可以反映信息量，我们可以用聚类的方法寻找信息密度集中的地方，然后挑选对应的数据。

比如绿色和紫色交界的地方，我们可以看到边界部分的点容易在绿色和紫色之间分类错误，这就类似于容易不小心做错的易错题，有学习细节的价值。这说明有高度不确定性的数据也有高的信息量，我们可以挑选这种边界数据来进行学习。

除此之外，主动学习还根据问题的特点定义了很多信息测量的方式，来试图寻找更重要、更有信息代表性的数据。

### 验证可信度-机器学习

机器学习的过程，即使从数据-标签中学习信息的过程，一个学习网络学习得到的能力接近于 $ I \le \bigcup signal(input, label) $，经验上说，I-O 的信息量越大，在模型能力足够、训练得当的前提下可以得到更好的学习能力。因此我们可以认为 
$$ model \propto \bigcup signal(input, label) $$

神经网络模型也好，硬件模型也好，软件模型也好，本质上就是一个 P。 而 P 的正确情况可以用 I-O 的正确情况来决定的。如果一个模块在测试的时候可以通过一组 input-label，则说明这个模块的正确性比学习了这组 input-label 的最优秀的神经网络来得更优秀的，所以待测模块的可信度高于最优秀的神经网络的可信度，进而高于 I-O 信息提供的可信度。
$$ module >= model$$

这里我们用 I-O 学习到的拟合模型刻画被验证模型的理论下界，随着 I-O 信息量的提高，模型的能力可以提高，进而被同样的 I-O 验证的模型的可信度下界也可以进一步提高。因此随着测试样例的信息量的提高，被验证的模块的可信度也会随之提高。

因此不仅仅在数据样本数目受限的情况下，机器学习要挑选信息量高的数据进行训练；在测试样例受限的情况下，测试工作也要选择信息量高的输入组合。

### 样例选择-数据选择

因此我们现在就可以将主动学习的信息量理论迁移到测试样例中来，尝试选择输入空间中信息量高的测试样例。可以看到，边界测试和随机测试组合的方法很好的符合了这个思路。

我们回到边界信息模型上来。从信息密度的角度上来看，每个区域中间的内容有较多重复的信息量，可以挑选部分内容进行训练，因此可以用随机测试的方法采样一部分；从信息不确定性的角度上来看，每个区域的边缘的内容有较多独立的信息量，可以尽可能多的进行测试，因此可以用边界测试的方法采样一部分。对应的，模糊测试等也符合这个信息量的选择规律。

### 联通信息模型-图关系

对于信息密度大的区域，我们说相邻的测试点具有类似的信息，因此重复选择之后新的信息量的提高不大，所以对这部分采取随机稀疏采样即可。这里，我们利用图理论建立一个比有界信息模型精细的联通信息模型，来对这个现象进行一个更直观的解释。

图主动学习的一些经验告诉我们，两个关系密切的节点，他们信息的相似度就越高。即使我们没有一个点的确切信息，也可以通过周围点的信息，和点之间的关系，还原出未知点的部分信息，并且关系越紧密，还原出的信息越多。例如我们有下图的图结构，已知 A 是 17, C 是 19，-> 是小于关系，那么即使我们不知道 B 的值，我们也可以根据图结果推断出 B 是 18。
```
    A -> B -> C
```
那么 B 的信息虽然没有采样学习，但是实际上已经学到了。也就是说我们学习一个点信息的时候，不仅仅学习了这个点的信息，也学习了和这个点有关联的点的部分信息。

对于下面这个真值表：

| I | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 
|---|---|---|---|---|---|---|---|---|
| O | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 0 |

如果我们用查表的方式实现，那么就是：
```systemverilog
module M(input [3:0] i, output o);
    wire [7:0] array = 8'b00111100;
    assign o = array[i];
endmodule
```

这个时候 0-7 这 7 个测试相互之间是没有关系的，所以测试之间的关系是 0，我们每个测试只能得到该测试点的信息，没有办法得到其他测试点的信息。下图为对应的联通信息模型，边上的权重表示点之间的互联通关系，上面的绿色柱子表示蕴含的信息量。

![对应的联通信息模型](img/connect-model-1.png)

这个时候我们采用更具有结构化的写法：
```systemverilog
module M(input [3:0] i, output o);
    always@(*)begin
        if(^i[2:1]) o = 1;
        else o = 0;
    end
endmodule
```
我们之前讲过 if-else 相当于将点分为两类，这种分类其实就是让点开始聚合，同类的点之间的关系会进一步变得密切，相连边的权重也会进一步加强；不同类的点之间的关系会进一步减弱，相连边的权重也会进一步下降。因此我们可以得到类似如下的联通信息模型。这里的边权重高的区域就对应了边界信息模型的区域，权重低的区域就对应了边界信息模型的边，并且关系从简单的边和内部变成了细化的权重。
![聚合后的联通信息模型](img/connect-model-2.png)

当我们测试了一个测试点之后，不仅仅学到了这个测试点的信息，还学到了相关点的信息，这里我们可以简单考虑边的权重，比如两个点之间的关系是0.9，那么一个点的 100% 信息被学习之后，另一个点的 90% 的信息也被学习了，所以这个点就只剩下了 10% 的可学习信息，信息量大大下降。这就解释了为什么对于同类型的点，当我们学习了一部分之后，其余部分的学习价值会大大下降。体现在 fuzz 上就是紧密关联的点可以提供的新的 coverage 大大下降等。
![采样后的信息量](img/connect-model-3.png)

我们在上文中说，组内部的点的信息量小于边，其实这里也可以调整一下说法，是因为组内部点关联度大，导致重复信息太多，从而导致虽然
$$not(signal(inner_i) < signal(edge_j))$$
但是平均可以提供的信息量
$$\frac{\bigcup_{i = 0}^N signal(inner_i)}{N} < \frac{\bigcup_{i = 0}^M signal(edge_i)}{M}$$
用这个模型即说明了类内点的信息量本身并不比边缘点的信息量少，给类内点应有的学习地位；同时也揭示了为什么类内点只需要采样学习的原因。




